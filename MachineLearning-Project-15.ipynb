{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch pandas openpyxl peft"
      ],
      "metadata": {
        "id": "ZsqpLCsfWcrP",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import json\n",
        "import torch\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "from peft import LoraConfig, get_peft_model, TaskType, PeftModel"
      ],
      "metadata": {
        "id": "pvvjNG7lBWra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"PyTorch Version: {torch.__version__}\")\n",
        "print(f\"CUDA Available: {torch.cuda.is_available()}\")"
      ],
      "metadata": {
        "id": "EQua5is0BWtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading KoGPT2 model and tokenizer...\")\n",
        "MODEL_NAME = \"skt/kogpt2-base-v2\" #ì‚¬ìš©í•  ëª¨ë¸ì˜ ì´ë¦„ ì €ì¥\n",
        "\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained( #ëª¨ë¸ì´ ì‚¬ìš©í•  ê·œì¹™ì„ í† í¬ë‚˜ì´ì €ë¥¼ ë¶ˆëŸ¬ì˜¨ë‹¤.\n",
        "    MODEL_NAME,\n",
        "    bos_token='</s>', eos_token='</s>', unk_token='<unk>',#ë¬¸ì¥ì— ì‹œì‘, ë, ëª¨ë¥´ëŠ” ë‹¨ì–´ ì²˜ë¦¬\n",
        "    pad_token='<pad>', mask_token='<mask>' #ê¸¸ì´ ë§ì¶¤, í•™ìŠµì„ í• ë•Œ ë¹ˆì¹¸ì„ ì±„ìš´ë‹¤.(ì‹¤ì œ ì‚¬ìš©X)\n",
        ")\n",
        "\n",
        "base_model = GPT2LMHeadModel.from_pretrained(MODEL_NAME) #í•™ìŠµëœ ê°€ì¤‘ì¹˜ ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "base_model.resize_token_embeddings(len(tokenizer)) #ì…ë ¥ì¸µ í¬ê¸°ë¥¼ í† í¬ë‚˜ì´ì €ì˜ ë‹¨ì–´ ìˆ˜ì— ë§ê²Œ ì¡°ì •\n",
        "\n",
        "print(\"Base model loaded.\")"
      ],
      "metadata": {
        "id": "KCX1kl2YWhr2",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    inference_mode=False,\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1\n",
        ")\n",
        "\n",
        "model = get_peft_model(base_model, lora_config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "CojDnDqLWhqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ChatbotDataset(Dataset):\n",
        "    def __init__(self, file_path, tokenizer, dataset_type):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = []\n",
        "        self.dataset_type = dataset_type\n",
        "        self.max_len = 256\n",
        "\n",
        "        print(f\"Loading data from: {file_path}\")\n",
        "\n",
        "        # [F]\n",
        "        if self.dataset_type == \"F_BOT\": #ë´‡ íƒ€ì…ì´ Fì¼ë•Œ ì‹¤í–‰\n",
        "            try: #ì—ëŸ¬ ë°œìƒì‹œ ì˜ˆì™¸ ì²˜ë¦¬\n",
        "                print(\"Parsing as F-Bot (JSON)...\")\n",
        "                with open(file_path, \"r\", encoding=\"utf-8\") as f: #ì§€ì •ëœ ê²½ë¡œì— jsoníŒŒì¼ì„ ì½ëŠ”ë‹¤.\n",
        "                    raw_data = json.load(f) #íŒŒì¼ ë‚´ìš©ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ë¡œ ë³€í™˜í•´ ë³€ìˆ˜ì— ì €ì¥\n",
        "\n",
        "                if isinstance(raw_data, dict): #ë°ì´í„°ê°€ {}ì¸ì§€ í˜•íƒœ í™•ì¸\n",
        "                    raw_data = raw_data.get('data', []) #{}ì´ë¼ë©´ dataí‚¤ ì•ˆì˜ ì‹¤ì œ ë°ì´í„°ë¥¼ êº¼ë‚¸ë‹¤.\n",
        "\n",
        "                count = 0 #ëŒ€í™” ìŒì„ ì„¸ê¸° ìœ„í•´ì„œ 0ìœ¼ë¡œ ì´ˆê¸°í™”\n",
        "                for item in tqdm(raw_data, desc=\"Processing Data\"):\n",
        "                    try: #ì˜ˆì™¸ ì²˜ë¦¬(ë°ì´í„°ì— ë¬¸ì œê°€ ë°œìƒí•˜ë©´ ë‹¤ìŒ ë°ì´í„°ë¡œ ë„˜ì–´ê°„ë‹¤.)\n",
        "                        if 'talk' not in item or 'content' not in item['talk']: #ë°ì´í„°ì— talkë‚˜ contentê°€ ì—†ìœ¼ë©´ ë¶ˆëŸ‰ íŒë‹¨\n",
        "                            continue #ë¶ˆëŸ‰ì´ë©´ ë‹¤ìŒ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¨ë‹¤.\n",
        "\n",
        "                        content = item['talk']['content'] #ì‹¤ì œëŒ€í™” ë‚´ìš©ì„ contentì— ì €ì¥\n",
        "                        i = 1 #ëŒ€í™” ìˆœì„œë¥¼ ë²ˆí˜¸ë¡œ ì„¤ì •\n",
        "                        while True: #í•œ ëŒ€í™” ì„¸ì…˜ì„ ëª¨ë‘ ì°¾ëŠ”ë‹¤.\n",
        "                            user_key = f\"HS0{i}\" #ëŒ€í™” ìˆœì„œì— ë”°ë¥¸ ì‚¬ìš©ì ì§ˆë¬¸ í‚¤ë¥¼ ë§Œë“ ë‹¤.\n",
        "                            bot_key = f\"SS0{i}\" #ëŒ€í™” ìˆœì„œì— ë”°ë¥¸ ì±—ë´‡ ë‹µë³€ í‚¤ë¥¼ ë§Œë“ ë‹¤.\n",
        "\n",
        "                            if user_key in content and bot_key in content: #ë‹µë³€ í‚¤ì™€ ì§ˆë¬¸ í‚¤ê°€ ë‘˜ ë‹¤ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸\n",
        "                                q = content[user_key] #ì‹¤ì œ ì§ˆë¬¸ ë‚´ìš©ë¥¼ ì €ì¥\n",
        "                                a = content[bot_key] #ì‹¤ì œ ë‹µë³€ ë‚´ìš©ì„ ì €ì¥\n",
        "                                if q and a: #ì‹¤ì œ ì§ˆë¬¸ê³¼ ë‹µë³€ì´ ë¹„ì–´ìˆì§€ ì•Šì€ì§€ í™•ì¸\n",
        "                                    formatted = f\"Q: {q}\\nA: {a}{tokenizer.eos_token}\" #Q: ì§ˆë¬¸ \\n A: ë‹µë³€ </s>í˜•ì‹ ë³€í˜•\n",
        "                                    self.data.append(formatted) #ì™„ì„±ëœ í•™ìŠµìš© ë¬¸ìì—´ì„ ìµœì¢… ë°ì´í„° ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
        "                                    count += 1 #ì¹´ìš´í„° ì¦ê°€\n",
        "                                i += 1 #ëŒ€í™” ìˆœì„œ ì¦ê°€\n",
        "                            else:\n",
        "                                break #ëŒ€í™” í„´ì´ ì—†ìœ¼ë©´ ì¢…ë£Œ\n",
        "                    except:\n",
        "                        continue\n",
        "\n",
        "                print(f\"Successfully loaded {count} dialogue pairs.\")\n",
        "\n",
        "            except Exception as e: #ì˜ˆì™¸ ì²˜ë¦¬\n",
        "                print(f\"Critical Error parsing F-Bot data: {e}\") #ì˜¤ë¥˜ ì¶œë ¥\n",
        "\n",
        "        # [T]\n",
        "        elif self.dataset_type == \"T_BOT\": #ë´‡ íƒ€ì…ì´ Tì¼ë•Œ ì‹¤í–‰\n",
        "            search_pattern = os.path.join(file_path, \"**\", \"*.json\") #ì§€ì •ëœ í´ë”ì—ì„œ í•˜ìœ„ í´ë”ì— ìˆëŠ” ëª¨ë“  jsoníŒŒì¼ ê²½ë¡œë¡œ ì§€ì •\n",
        "            json_files = glob.glob(search_pattern, recursive=True) #ëª¨ë“  jsoníŒŒì¼ì„ ì°¾ì•„ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥\n",
        "            for json_file in tqdm(json_files, desc=\"Parsing T-Bot files\"): #íŒŒì¼ ëª©ë¡ì„ í•˜ë‚˜ì”© êº¼ë‚¸ë‹¤.\n",
        "                try: #ì˜ˆì™¸ ì²˜ë¦¬\n",
        "                    with open(json_file, \"r\", encoding=\"utf-8\") as f: #íŒŒì¼ì„ ëª¨ë‘ ì½ê¸° ëª¨ë“œë¡œ ì—°ë‹¤.\n",
        "                        raw_data = json.load(f) #json ë‚´ìš©ì„ ë°ì´í„°ë¡œ ë³€í™˜\n",
        "                    if 'paragraph' in raw_data: #ì‹¬ë¦¬ìƒë‹´ ë°ì´í„°ì˜ paragraphí‚¤ê°€ ìˆëŠ”ì§€ í™•ì¸\n",
        "                        dialogue = raw_data['paragraph'] #ëŒ€í™” ë‚´ìš©ì´ ë‹´ê¸´ ë¦¬ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜¨ë‹¤.\n",
        "                        for i in range(len(dialogue) - 1): #ë§ˆì§€ë§‰ í„´ì€ ë‹µë³€ì´ ì—†ìœ¼ë¯€ë¡œ -1ì„ í•œë‹¤.\n",
        "                            curr = dialogue[i] #í˜„ì¬ ëŒ€í™”\n",
        "                            next_t = dialogue[i+1] #ë‹¤ìŒ ëŒ€í™”\n",
        "                            if curr.get('paragraph_speaker') == \"ë‚´ë‹´ì\" and next_t.get('paragraph_speaker') == \"ìƒë‹´ì‚¬\": #ì§ˆë¬¸ìê°€ ë‚´ë‹´ìì´ê³  ë‹µë³€ìê°€ ìƒë‹´ìì¸ ê²½ìš°ë§Œ ì¶”ì¶œ\n",
        "                                q = curr.get('paragraph_text') #ë‚´ë‹´ì(ì§ˆë¬¸) ì¶”ì¶œ\n",
        "                                a = next_t.get('paragraph_text') #ìƒë‹´ì(ë‹µë³€) ì¶”ì¶œ\n",
        "                                if q and a: #ì‹¤ì œ ì§ˆë¬¸ê³¼ ë‹µë³€ì´ ë¹„ì–´ìˆì§€ ì•Šì€ì§€ í™•ì¸\n",
        "                                    formatted = f\"Q: {q}\\nA: {a}{tokenizer.eos_token}\" #Q: ì§ˆë¬¸ \\n A: ë‹µë³€ </s>í˜•ì‹ ë³€í˜•\n",
        "                                    self.data.append(formatted)#ì™„ì„±ëœ í•™ìŠµìš© ë¬¸ìì—´ì„ ìµœì¢… ë°ì´í„° ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
        "                except:\n",
        "                    pass #íŒŒì¼ì— ì—ëŸ¬ê°€ ë‚˜ë©´ ë‹¤ìŒ íŒŒì¼ë¡œ ë„˜ì–´ê°„ë‹¤.\n",
        "\n",
        "        print(f\"Total processed data entries: {len(self.data)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.data[idx]\n",
        "        tokens = self.tokenizer(\n",
        "            text, padding=\"max_length\", truncation=True,\n",
        "            max_length=self.max_len, return_tensors=\"pt\"\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": tokens[\"input_ids\"].squeeze(),\n",
        "            \"attention_mask\": tokens[\"attention_mask\"].squeeze()\n",
        "        }"
      ],
      "metadata": {
        "id": "6fy5OeHEWhou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def __len__(self):\n",
        "        return len(self.data)"
      ],
      "metadata": {
        "id": "6pw8gMxPWhnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def __getitem__(self, idx):\n",
        "        text = self.data[idx]\n",
        "        tokens = self.tokenizer(\n",
        "            text, padding=\"max_length\", truncation=True,\n",
        "            max_length=self.max_len, return_tensors=\"pt\"\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": tokens[\"input_ids\"].squeeze(),\n",
        "            \"attention_mask\": tokens[\"attention_mask\"].squeeze()\n",
        "        }"
      ],
      "metadata": {
        "id": "0wpB0imlWhle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BOT_TYPE = \"F_BOT\"   # Fë´‡ í•™ìŠµ ì‹œ\n",
        "#BOT_TYPE = \"T_BOT\"   # Të´‡ í•™ìŠµ ì‹œ\n",
        "\n",
        "DRIVE_BASE_PATH = \"/content/drive/My Drive/Dataset\"\n",
        "MODEL_SAVE_DIR = \"/content/drive/My Drive/Dataset\"\n",
        "\n",
        "if BOT_TYPE == \"F_BOT\":\n",
        "    DATA_FILE_PATH = os.path.join(DRIVE_BASE_PATH, \"ê°ì„±ëŒ€í™”ë§ë­‰ì¹˜(ìµœì¢…ë°ì´í„°)_Training.json\")\n",
        "    SAVE_MODEL_PATH = os.path.join(MODEL_SAVE_DIR, \"kogpt2_F_bot_LoRA\")\n",
        "else:\n",
        "    DATA_FILE_PATH = os.path.join(DRIVE_BASE_PATH, \"ë¼ë²¨ë§ë°ì´í„°\")\n",
        "    SAVE_MODEL_PATH = os.path.join(MODEL_SAVE_DIR, \"kogpt2_T_bot_LoRA\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "learning_rate = 3e-5\n",
        "epochs = 3\n",
        "batch_size = 4"
      ],
      "metadata": {
        "id": "I5o_jeArWqH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, dataset, optimizer, device):\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "    model.to(device)\n",
        "\n",
        "    print(f\"Starting training for {BOT_TYPE}...\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train() #í•™ìŠµ ëª¨ë“œ ì „í™˜\n",
        "        total_loss = 0 #ì†ì‹¤ ì´ˆê¸°í™”\n",
        "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\") #ì§„í–‰ë¥  í‘œì‹œ\n",
        "\n",
        "        for batch in progress_bar: #ë°ì´í„°ë¥¼ batchì”© êº¼ë‚´ì™€ í•™ìŠµ ì§„í–‰\n",
        "            input_ids = batch['input_ids'].to(device) #ì§ˆë¬¸ê³¼ ë‹µë³€ì´ ìˆ«ìë¡œ ë³€í™˜ëœ ë°ì´í„°ë¥¼ GPUë¡œ ì˜®ê¸´ë‹¤.\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = input_ids.clone() #ì…ë ¥ ë°ì´í„° ë³µì‚¬ë³¸ì„ ì •ë‹µìœ¼ë¡œ ì‚¬ìš©\n",
        "\n",
        "            optimizer.zero_grad() #ê¸°ìš¸ê¸° ì´ˆê¸°í™”\n",
        "\n",
        "            outputs = model( #ìˆœì „íŒŒ ì§„í–‰\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels\n",
        "            )\n",
        "\n",
        "            loss = outputs.loss #ì˜¤ì°¨ ê³„ì‚°\n",
        "            loss.backward() #ì—­ì „íŒŒ ì§„í–‰\n",
        "            optimizer.step() #ê¸°ìš¸ê¸°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸\n",
        "\n",
        "            total_loss += loss.item() #ì „ì²´ ì˜¤ì°¨ë¥¼ êµ¬í•œë‹¤.\n",
        "            progress_bar.set_postfix({'loss': loss.item()}) #ì§„í–‰ë¥  í‘œì‹œ\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader) #í‰ê·  ì˜¤ì°¨ ê³„ì‚°\n",
        "        perplexity = torch.exp(torch.tensor(avg_loss)).item() #í¼í”Œë ‰ì‹œí‹° ê³„\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | Avg Loss: {avg_loss:.4f} | Perplexity: {perplexity:.2f}\")\n",
        "\n",
        "    print(\"Training finished.\")"
      ],
      "metadata": {
        "id": "dVsNiaCPWqFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Google Drive mounted.\")\n",
        "    os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
        "except ImportError:\n",
        "    print(\"Not in Colab.\")\n",
        "\n",
        "try:\n",
        "    dataset = ChatbotDataset(DATA_FILE_PATH, tokenizer, BOT_TYPE)\n",
        "\n",
        "    if len(dataset) > 0:\n",
        "        optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "        train(model, dataset, optimizer, device)\n",
        "\n",
        "        print(f\"Saving LoRA model to {SAVE_MODEL_PATH}...\")\n",
        "        model.save_pretrained(SAVE_MODEL_PATH)\n",
        "        tokenizer.save_pretrained(SAVE_MODEL_PATH)\n",
        "        print(\"LoRA Model saved successfully!\")\n",
        "    else:\n",
        "        print(\"ERROR: Dataset is empty. Check your file paths.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "id": "6YXgyXYMWtfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat(model_path, user_input):\n",
        "    if not os.path.exists(model_path):\n",
        "        return \"ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\"\n",
        "\n",
        "    print(\"Loading LoRA model for chat...\")\n",
        "\n",
        "    base = GPT2LMHeadModel.from_pretrained(MODEL_NAME)\n",
        "    lora_model = PeftModel.from_pretrained(base, model_path)\n",
        "\n",
        "    tokenizer = PreTrainedTokenizerFast.from_pretrained(model_path)\n",
        "    lora_model.to(device)\n",
        "    lora_model.eval()\n",
        "\n",
        "    prompt = f\"Q: {user_input}\\nA:\"\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = lora_model.generate(\n",
        "            input_ids, max_length=128, pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id, do_sample=True,\n",
        "            top_p=0.95, top_k=50, repetition_penalty=1.2\n",
        "        )\n",
        "\n",
        "    text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    try:\n",
        "        return text.split(\"A:\")[1].strip()\n",
        "    except:\n",
        "        return text"
      ],
      "metadata": {
        "id": "QmIPecPtWteF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\n--- {BOT_TYPE} Chat Test ---\")\n",
        "try:\n",
        "    test_inputs = [\n",
        "        \"ì˜¤ëŠ˜ ë„ˆë¬´ ìš°ìš¸í•´ì„œ ì•„ë¬´ê²ƒë„ í•˜ê¸° ì‹«ì–´\",\n",
        "        \"ìš”ì¦˜ ì ì„ ì˜ ëª» ìëŠ” ê²ƒ ê°™ì•„\",\n",
        "        \"ë¯¸ë˜ê°€ ë„ˆë¬´ ë¶ˆì•ˆí•´\"\n",
        "    ]\n",
        "\n",
        "    for q in test_inputs:\n",
        "        print(f\"ë‚˜: {q}\")\n",
        "        print(f\"ë´‡: {chat(SAVE_MODEL_PATH, q)}\\n\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Test Error: {e}\")"
      ],
      "metadata": {
        "id": "t9tVJwOCWtZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import peft\n",
        "except ImportError:\n",
        "    !pip install transformers torch peft\n",
        "\n",
        "import torch\n",
        "import os\n",
        "from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel\n",
        "from peft import PeftModel\n",
        "from google.colab import drive\n",
        "\n",
        "# 2. êµ¬ê¸€ ë“œë¼ì´ë¸Œ ì—°ê²°\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "except:\n",
        "    print(\"ì´ë¯¸ ì—°ê²°ë˜ì–´ ìˆê±°ë‚˜ ë¡œì»¬ í™˜ê²½ì…ë‹ˆë‹¤.\")\n",
        "\n",
        "# 3. ì„¤ì •\n",
        "BASE_PATH = \"/content/drive/My Drive/Dataset\"\n",
        "F_BOT_PATH = os.path.join(BASE_PATH, \"kogpt2_F_bot_LoRA\")\n",
        "T_BOT_PATH = os.path.join(BASE_PATH, \"kogpt2_T_bot_LoRA\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "MODEL_NAME = \"skt/kogpt2-base-v2\"\n",
        "\n",
        "print(\"ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘... (ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë”©)\")\n",
        "\n",
        "try:\n",
        "    base_model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)\n",
        "    base_model.to(device)\n",
        "    print(\"ë² ì´ìŠ¤ ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ!\")\n",
        "except Exception as e:\n",
        "    print(f\"ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
        "    exit()\n",
        "\n",
        "def get_chatbot(mode):\n",
        "    if mode == \"F\":\n",
        "        target_path = F_BOT_PATH\n",
        "        name = \"F-Bot (ê³µê°í˜•)\"\n",
        "    else:\n",
        "        target_path = T_BOT_PATH\n",
        "        name = \"T-Bot (ìƒë‹´í˜•)\"\n",
        "\n",
        "    if not os.path.exists(target_path):\n",
        "        print(f\"ì˜¤ë¥˜: {target_path} ê²½ë¡œì— ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "        return None, None\n",
        "\n",
        "    print(f\"\\n[{name}] ë¡œ ì¥ì°© ì¤‘...\")\n",
        "    model = PeftModel.from_pretrained(base_model, target_path)\n",
        "    tokenizer = PreTrainedTokenizerFast.from_pretrained(target_path)\n",
        "    model.eval()\n",
        "    return model, tokenizer\n",
        "\n",
        "def chat_system():\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(\"MBTI ì„±í–¥ ì „í™˜ ì±—ë´‡ (F â†” T)\")\n",
        "    print(\"ëª…ë ¹ì–´: '!ë³€ê²½'ìœ¼ë¡œ ì„±ê²© ì „í™˜, '!ì¢…ë£Œ'ë¡œ ëë‚´ê¸°\")\n",
        "    print(\"=\"*40)\n",
        "\n",
        "    current_mode = \"F\" #ì´ˆê°€ì—ëŠ” F(ê³µê°í˜•)ë¡œ ì„¤ì •\n",
        "    model, tokenizer = get_chatbot(current_mode) #í† ê·¸ë‚˜ì´ì €ë¥¼ ë¶ˆëŸ¬ì˜¨ë‹¤.\n",
        "    if model is None: return\n",
        "\n",
        "    while True:\n",
        "        bot_label = \"F(ê³µê°)\" if current_mode == \"F\" else \"T(ìƒë‹´)\" #F(ê³µê°)ë˜ëŠ” T(ìƒë‹´)ì„ íƒ\n",
        "        user_input = input(f\"\\nğŸ‘¤ ë‚˜: \") #ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë‚´ìš©ì„ ì €ì¥\n",
        "\n",
        "        if user_input.strip() in [\"!ì¢…ë£Œ\", \"!quit\", \"ì¢…ë£Œ\"]: #ì¢…ë£Œ ëª…ë ¹ì–´ ì…ë ¥ì‹œì— ìƒë‹´ ì¢…ë£Œ\n",
        "            print(\"ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
        "            break\n",
        "\n",
        "        if user_input.strip() in [\"!ë³€ê²½\", \"!switch\", \"ë³€ê²½\"]: #ì„±ê²© ë³€ê²½ ëª…ë ¹ì–´ ì…ë ¥ì‹œ ì„±ê²© ë³€í™˜\n",
        "            current_mode = \"T\" if current_mode == \"F\" else \"F\"\n",
        "            print(f\"\\nì„±ê²©ì„ ì „í™˜í•©ë‹ˆë‹¤! ({'F' if current_mode=='T' else 'T'} -> {current_mode})\")\n",
        "            model, tokenizer = get_chatbot(current_mode)\n",
        "            continue\n",
        "\n",
        "        if not user_input: continue\n",
        "\n",
        "        prompt = f\"Q: {user_input}\\nA:\" #í•™ìŠµ ì‚¬ìš©í•œ í˜•ì‹ìœ¼ë¡œ ë³€ê²½í•´ ë‹µë³€ ì¤€ë¹„\n",
        "        input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device) #ì§ˆë¬¸ì„ ìˆ«ìë¡œ ë³€í™˜\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model.generate(\n",
        "                input_ids,\n",
        "                max_length=256,\n",
        "                min_length=32,\n",
        "                pad_token_id=tokenizer.pad_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id,\n",
        "                do_sample=True,\n",
        "                top_k=50,\n",
        "                top_p=0.92,\n",
        "                repetition_penalty=1.1,\n",
        "                no_repeat_ngram_size=3,\n",
        "                temperature=0.7\n",
        "            )\n",
        "\n",
        "        text = tokenizer.decode(output[0], skip_special_tokens=True) #ìˆ«ìë¥¼ ë‹¤ì‹œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
        "        try:\n",
        "            response = text.split(\"A:\")[-1].strip() #ì‹¤ì œ ë‹µë³€ë§Œ ì˜ë¼ë‚¸ë‹¤.\n",
        "        except:\n",
        "            response = text\n",
        "\n",
        "        response = response.replace(\"NAME\", \"ì‚¬ìš©ì\") #ë‹µë³€ì— ìˆëŠ” ë‹¨ì–´ë¥¼ êµì²´\n",
        "        response = response.replace(\"name\", \"ì‚¬ìš©ì\")\n",
        "        response = response.replace(\"PLACE\", \"ì±—ë´‡\")\n",
        "        response = response.replace(\"place\", \"ì±—ë´‡\")\n",
        "\n",
        "        print(f\"ğŸ¤– {bot_label} ë´‡: {response}\") #ë‹µë³€ ì¶œë ¥\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    chat_system()"
      ],
      "metadata": {
        "id": "8gZ4XpyHMWTo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}